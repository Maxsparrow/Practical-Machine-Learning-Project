<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Practical-machine-learning-project : ">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Practical-machine-learning-project</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/Maxsparrow/Practical-Machine-Learning-Project">View on GitHub</a>

          <h1 id="project_title">Practical-machine-learning-project</h1>
          <h2 id="project_tagline"></h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/Maxsparrow/Practical-Machine-Learning-Project/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/Maxsparrow/Practical-Machine-Learning-Project/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <p></p>

<p>

</p>

<p></p>

<p></p>Practical Machine Learning Project



<p>

</p>



code{white-space: pre;}

<p></p>




  pre:not([class]) {
    background-color: white;
  }




<p></p>

<p></p>


.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}


<div>


<div id="header">
<h1>
<a id="practical-machine-learning-project" class="anchor" href="#practical-machine-learning-project" aria-hidden="true"><span class="octicon octicon-link"></span></a>Practical Machine Learning Project</h1>
<h4>
<a id="chris-johnson" class="anchor" href="#chris-johnson" aria-hidden="true"><span class="octicon octicon-link"></span></a><em>Chris Johnson</em>
</h4>
<h4>
<a id="monday-december-15-2014" class="anchor" href="#monday-december-15-2014" aria-hidden="true"><span class="octicon octicon-link"></span></a><em>Monday, December 15, 2014</em>
</h4>
</div>

<p>First let’s load the data and set a seed for reproducibility.</p>

<pre><code>library(caret)</code></pre>

<pre><code>## Loading required package: lattice
## Loading required package: ggplot2</code></pre>

<pre><code>originalset&lt;-read.csv("~/Data Scientist Specialization/Practical Machine Learning/pml-training.csv")
problemset&lt;-read.csv("~/Data Scientist Specialization/Practical Machine Learning/pml-testing.csv")
set.seed(1234)</code></pre>

<p>After reading in the trainset and test set, let’s split into a trainset and testset.</p>

<pre><code>originalset$skewness_pitch_dumbbell&lt;-as.numeric(as.character(originalset$skewness_pitch_dumbbell))</code></pre>

<pre><code>## Warning: NAs introduced by coercion</code></pre>

<pre><code>inTrain&lt;-createDataPartition(y=originalset$classe,p=0.6,list=FALSE)
trainset&lt;-originalset[inTrain,]
testset&lt;-originalset[-inTrain,]</code></pre>

<p>Now let’s look at the variables in the trainset to see what we might want to use for prediction.</p>

<p>The first variable looks like an index. The next few are user names and timestamps, which we won’t want to include in our model. It’s also not immediately clear what new_window and num_window are so we should leave those out. The rest of the variables seem like good candidates for a model, except some variables only have values of #DIV/0 or blanks. So I’m going to go ahead and remove those from the analysis</p>

<pre><code>colstokeep&lt;-vector()
for(i in 1:ncol(trainset)) {
    divcheck&lt;-grep("DIV",trainset[,i])
    if(length(divcheck)==0){colstokeep&lt;-c(colstokeep,i)}
}
trainset&lt;-trainset[,colstokeep]</code></pre>

<p>I’m now going to remove any columns that have NA values too.</p>

<pre><code>colstokeep&lt;-vector()
for(i in 1:ncol(trainset)) {
    NAcheck&lt;-sum(is.na(trainset[,i]))==0
    if(NAcheck){colstokeep&lt;-c(colstokeep,i)}
}
trainset&lt;-trainset[,colstokeep]</code></pre>

<p>Let’s try fitting a model now using a Random Forest.</p>

<pre><code>library(randomForest)</code></pre>

<pre><code>## randomForest 4.6-10
## Type rfNews() to see new features/changes/bug fixes.</code></pre>

<pre><code>fit&lt;-randomForest(classe~.-X-user_name-raw_timestamp_part_1-raw_timestamp_part_2-cvtd_timestamp-new_window-num_window,data=trainset)
confusionMatrix(predict(fit),trainset$classe)</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 3344   16    0    0    0
##          B    2 2259   17    0    0
##          C    0    4 2035   23    3
##          D    0    0    2 1905    6
##          E    2    0    0    2 2156
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9935          
##                  95% CI : (0.9918, 0.9948)
##     No Information Rate : 0.2843          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9917          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9988   0.9912   0.9907   0.9870   0.9958
## Specificity            0.9981   0.9980   0.9969   0.9992   0.9996
## Pos Pred Value         0.9952   0.9917   0.9855   0.9958   0.9981
## Neg Pred Value         0.9995   0.9979   0.9980   0.9975   0.9991
## Prevalence             0.2843   0.1935   0.1744   0.1639   0.1838
## Detection Rate         0.2840   0.1918   0.1728   0.1618   0.1831
## Detection Prevalence   0.2853   0.1934   0.1754   0.1624   0.1834
## Balanced Accuracy      0.9985   0.9946   0.9938   0.9931   0.9977</code></pre>

<p>The in sample error is very low, at less than 0.65%.</p>

<pre><code>confusionMatrix(predict(fit,newdata=testset),testset$classe)</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 2232   10    0    0    0
##          B    0 1503    9    0    0
##          C    0    5 1356   19    2
##          D    0    0    3 1265    3
##          E    0    0    0    2 1437
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9932          
##                  95% CI : (0.9912, 0.9949)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9915          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            1.0000   0.9901   0.9912   0.9837   0.9965
## Specificity            0.9982   0.9986   0.9960   0.9991   0.9997
## Pos Pred Value         0.9955   0.9940   0.9812   0.9953   0.9986
## Neg Pred Value         1.0000   0.9976   0.9981   0.9968   0.9992
## Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
## Detection Rate         0.2845   0.1916   0.1728   0.1612   0.1832
## Detection Prevalence   0.2858   0.1927   0.1761   0.1620   0.1834
## Balanced Accuracy      0.9991   0.9943   0.9936   0.9914   0.9981</code></pre>

<p>The out of sample error can be estimated to be about 0.68%. It is probably somewhat higher though, since a Random Forest is a very complex model, that could have overfit the data.</p>

<p></p>
</div>






      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Practical-machine-learning-project maintained by <a href="https://github.com/Maxsparrow">Maxsparrow</a></p>
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
